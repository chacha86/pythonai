{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbc+d+Mq5SqFQ+VuUALRA2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chacha86/pythonai/blob/main/chatbot_with_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaVkpAPhtC24"
      },
      "outputs": [],
      "source": [
        "!pip install Korpora"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ProeiBN9krPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/ChatbotData.csv')\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_Qq0EA6k4y2",
        "outputId": "8e5c4910-4ed0-4193-d5e2-d0f5b812b19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11823 entries, 0 to 11822\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Q       11823 non-null  object\n",
            " 1   A       11823 non-null  object\n",
            " 2   label   11823 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 277.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문과 답변 문장을 따로 저장\n",
        "\n",
        "texts = []\n",
        "pairs = []\n",
        "for text, pair in zip(df['Q'], df['A']) :\n",
        "  texts.append(text)\n",
        "  pairs.append(pair)\n"
      ],
      "metadata": {
        "id": "H4M-Y1MHk8cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문과 답변 쌍을 5개 확인\n",
        "list(zip(texts, pairs))[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EedynmK6jTNs",
        "outputId": "11ced055-e618-4e2b-e90f-a6acaac69f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('12시 땡!', '하루가 또 가네요.'),\n",
              " ('1지망 학교 떨어졌어', '위로해 드립니다.'),\n",
              " ('3박4일 놀러가고 싶다', '여행은 언제나 좋죠.'),\n",
              " ('3박4일 정도 놀러가고 싶다', '여행은 언제나 좋죠.'),\n",
              " ('PPL 심하네', '눈살이 찌푸려지죠.')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 빠르고 간단한 테스트를 위해 특수문자와 영어 제거\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_sentence(sentence) :\n",
        "  sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]', r'', sentence)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "TZEXQSt8kedc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 위 함수 잘 작동하는지 테스트\n",
        "sentence = clean_sentence('12시 땡^^!!??')\n"
      ],
      "metadata": {
        "id": "hBZKlLXkm0BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 한국어 문장을 분해하기 위한 라이브러리(형태소 분석)\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "Au7hJwb2nGBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4705bf-028a-4e04-bb76-1566f3d31171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 형태소 추출\n",
        "### 형태소란? 의미를 가지는 요소로서는 더 이상 쪼갤 수 없는 가장 작은 말의 단위\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "def process_morph(sentence):\n",
        "  return ' '.join(okt.morphs(sentence))\n"
      ],
      "metadata": {
        "id": "YCwGsSXFnS-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 위 함수 잘 작동하는지 확인\n",
        "process_morph('안녕하세요 저는 홍길동입니다. 당신의 성공을 항상 기원합니다. 사랑합니다.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gwzDsX3fnq3q",
        "outputId": "0b90b4ba-888e-46fd-8c12-faecb7683ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요 저 는 홍길동 입니다 . 당신 의 성공 을 항상 기원 합니다 . 사랑 합니다 .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 문장을 입력받아 형태소로 쪼개주는 함수\n",
        "def clean_and_morph(sentence, is_question=True):\n",
        "  ## 한글만 남기기\n",
        "  sentence = clean_sentence(sentence)\n",
        "\n",
        "  ## 형태소로 쪼개기\n",
        "  sentence = process_morph(sentence)\n",
        "\n",
        "  if is_question:\n",
        "    return sentence\n",
        "\n",
        "  else :\n",
        "    ## 후에 토크나이저하기 위해서는 공백이 꼭 들어가야 함.\n",
        "    return ('<START> ' + sentence, sentence + ' <END>')"
      ],
      "metadata": {
        "id": "5q1lF7BIoTYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(texts, pairs):\n",
        "  questions = []\n",
        "  answer_in = []\n",
        "  answer_out = []\n",
        "\n",
        "  ## 질문에 대한 전처리\n",
        "  for text in texts :\n",
        "    question = clean_and_morph(text, is_question=True)\n",
        "    questions.append(question)\n",
        "\n",
        "  ## 답변에 대한 전처리\n",
        "  for pair in pairs:\n",
        "    in_, out_ = clean_and_morph(pair, is_question=False)\n",
        "    answer_in.append(in_)\n",
        "    answer_out.append(out_)\n",
        "\n",
        "  return questions, answer_in, answer_out\n"
      ],
      "metadata": {
        "id": "_vZHmSmxo1KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q, ai, ao = preprocess(texts[:3], pairs[:3])\n"
      ],
      "metadata": {
        "id": "04hrKTtqLcFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q\n",
        "ai\n",
        "ao\n",
        "\n",
        "len(questions)"
      ],
      "metadata": {
        "id": "RnSkvJ_RLuD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = texts[:1000]\n",
        "pairs = pairs[:1000]\n",
        "questions, answer_in, answer_out = preprocess(texts, pairs)"
      ],
      "metadata": {
        "id": "6nNvVAx4pXjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXppn2WJpepA",
        "outputId": "22988d52-1615-4ad6-fecf-a214ab1ab64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['12시 땡', '1 지망 학교 떨어졌어', '3 박 4일 놀러 가고 싶다', '3 박 4일 정도 놀러 가고 싶다', '심하네']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_in[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLyyL-3Zphbc",
        "outputId": "ed7d0e16-bbca-4521-cda9-5550cd6eaf03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START> 하루 가 또 가네요',\n",
              " '<START> 위로 해 드립니다',\n",
              " '<START> 여행 은 언제나 좋죠',\n",
              " '<START> 여행 은 언제나 좋죠',\n",
              " '<START> 눈살 이 찌푸려지죠']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_out[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxz4kyv5phdi",
        "outputId": "6ec6d412-6529-4c54-8cd4-347b19481937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['하루 가 또 가네요 <END>',\n",
              " '위로 해 드립니다 <END>',\n",
              " '여행 은 언제나 좋죠 <END>',\n",
              " '여행 은 언제나 좋죠 <END>',\n",
              " '눈살 이 찌푸려지죠 <END>']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 후에 토크나이저를 한번에 하기 위해 문장을 합쳐줌(리스트 합치기)\n",
        "all_sentences = questions + answer_in + answer_out"
      ],
      "metadata": {
        "id": "sV9XUE4fp1ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sentences[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "G68I99HMMVlr",
        "outputId": "140138d5-c2aa-45e8-a3e5-57e7c08a18c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'신나는 노래 로 분위기 를 띄어 보세요 <END>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 형태소 개수\n",
        "a = (' '.join(questions) + ' '.join(answer_in) + ' '.join(answer_out)).split()\n",
        "len(set(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_BRNF3pp7Xz",
        "outputId": "2e8f9818-3e35-4f73-f7e3-53ac5cf87194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2300"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "UcYFZGv5t_90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## filter => 문장의 특수기호등을 임의로 처리하지 말라. 필터링 하지 말라\n",
        "## lower => 소문자로 변경하지 마라\n",
        "## oov_token => 단어 사전에 존재하지 않는 단어라면 '<OOV>'로 대체\n",
        "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')"
      ],
      "metadata": {
        "id": "ugv_J-TauQ0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 단어 사전 만들기\n",
        "## 공백을 기준으로 쪼개주는 듯하다\n",
        "tokenizer.fit_on_texts(all_sentences)"
      ],
      "metadata": {
        "id": "r7KNJBtyueTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 각 단어와 단어의 인덱스 번호를 확인\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "  print(f'{word}\\t\\t => \\t{idx}')\n",
        "  if idx > 10:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr4XNnjuum6l",
        "outputId": "e5ea7aef-95f3-46b6-934b-7920819f8c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<OOV>\t\t => \t1\n",
            "<START>\t\t => \t2\n",
            "<END>\t\t => \t3\n",
            "이\t\t => \t4\n",
            "거\t\t => \t5\n",
            "을\t\t => \t6\n",
            "가\t\t => \t7\n",
            "나\t\t => \t8\n",
            "예요\t\t => \t9\n",
            "사람\t\t => \t10\n",
            "요\t\t => \t11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "VOCAB_SIZE"
      ],
      "metadata": {
        "id": "lpGi1y4qu87k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0600b6f0-8fcd-4e66-f31d-4a70289f1854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2301"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 문자형태를 숫자 형태로 바꾸기\n",
        "question_sequence = tokenizer.texts_to_sequences(questions)\n",
        "answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n",
        "answer_out_sequence = tokenizer.texts_to_sequences(answer_out)"
      ],
      "metadata": {
        "id": "KvqXzpf3vDkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 문자가 숫자로 바뀐 것 확인\n",
        "questions[0], answer_in_sequence[0],answer_out_sequence[0], pairs[0], tokenizer.word_index['하루']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz0r5S1qvX5l",
        "outputId": "d2ba9f90-7aac-42ad-c534-0e34ef3a0af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('12시 땡', [2, 391, 7, 356, 1234], [391, 7, 356, 1234, 3], '하루가 또 가네요.', 391)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 딥러닝의 경우 입력값이 항상 일정해야 하므로(네트워크 모델은 입력값에 의해 모양이 바뀌므로 입력값은 바뀌면 안된다.)\n",
        "MAX_LENGTH = 30 # 최대 몇개의 단어\n",
        "TRUNCATING = 'post' # 잘라낼 때 앞(pre), 뒤(post)\n",
        "PADDING = 'post' # 채워줄 때 앞(pre), 뒤(post)"
      ],
      "metadata": {
        "id": "y1crUxZ5vq7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 트런케이팅과 패딩 적용하기\n",
        "question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "answer_in_padded = pad_sequences(answer_in_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "answer_out_padded = pad_sequences(answer_out_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)"
      ],
      "metadata": {
        "id": "7nQ7HaCBvxQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_padded.shape, answer_in_padded.shape, answer_out_padded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4lr_MHKwH6b",
        "outputId": "c4a5e018-202b-4233-a7de-720469f24d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 30), (1000, 30), (1000, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_padded[0]\n",
        "\n",
        "#tokenizer.word_index['12시']\n",
        "#tokenizer.word_index['땡']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_fRfyZfwQ2C",
        "outputId": "4504c8cb-3729-4fe1-98f1-86b141c3488c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1608, 1609,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 단어에는 비교우위가 없으므로 카테고리컬로 데이터로 보고 원핫 인코딩을 해준다.\n",
        "def convert_to_one_hot(padded) :\n",
        "  one_hot_vector = np.zeros((len(padded), MAX_LENGTH, VOCAB_SIZE))\n",
        "\n",
        "  for i, sequence in enumerate(padded):\n",
        "    for j, index in enumerate(sequence):\n",
        "      one_hot_vector[i, j, index] = 1\n",
        "\n",
        "  return one_hot_vector\n",
        "\n",
        "answer_in_padded[:5]"
      ],
      "metadata": {
        "id": "AF7xAnt6wbSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb93cac-37fb-410e-d4af-880986efbceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2,  391,    7,  356, 1234,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   2,  143,   36,  414,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   2,  106,   19,  206,   86,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   2,  106,   19,  206,   86,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   2, 1235,    4, 1236,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index.values()"
      ],
      "metadata": {
        "id": "y4--eBy1W9IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 우선, 단어 인덱스의 최대값을 찾아야 합니다.\n",
        "# 이 값이 원-핫 벡터의 길이가 됩니다.\n",
        "max_word_idx = 12637  # `data`는 단어 인덱스를 담고 있는 (11823, 30) 형태의 numpy 배열입니다.\n",
        "\n",
        "# 원-핫 인코딩 수행\n",
        "data_one_hot = to_categorical(answer_in_padded, num_classes=max_word_idx + 1)\n"
      ],
      "metadata": {
        "id": "ye1-0Rf8VAcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np.equal(data_one_hot, answer_in_one_hot)\n",
        "#data_one_hot.shape, answer_in_one_hot.shape"
      ],
      "metadata": {
        "id": "IUuXlGrsWQie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_in_one_hot = convert_to_one_hot(answer_in_padded)\n",
        "answer_out_one_hot = convert_to_one_hot(answer_out_padded)"
      ],
      "metadata": {
        "id": "19hC1yD7z4Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_in_one_hot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr4iJMrgSUGe",
        "outputId": "574f6d4a-c9e9-4b6a-b847-9979a99fda02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 30, 2301)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_in_one_hot[0].shape, answer_out_one_hot[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg8OMiBZzsy3",
        "outputId": "edd1d43d-48d4-4a45-c96b-d86dd757517a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((30, 2301), (30, 2301))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 모델이 예측한 인코딩된 값을 다시 문자로 디코딩 해주는 함수\n",
        "def convert_index_to_text(indexes, end_token):\n",
        "  sentence = ''\n",
        "  for index in indexes: ## 문장의 순서\n",
        "    if index == end_token:  ## 문장의 마지막이면 종료\n",
        "      break\n",
        "    if index > 0 and tokenizer.index_word[index] is not None: ## 단아 사전에 존재하고 올바른 인덱스라면\n",
        "      sentence += tokenizer.index_word[index] # 최종 문자열에 이어 붙인다.\n",
        "    else:\n",
        "      sentence += '' # 없는 거면 공백.\n",
        "\n",
        "    sentence += ' ' # 한 형태소가 끝나면 띄어쓰기\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "_ycnEXP30B8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ],
      "metadata": {
        "id": "-vPxMg4a07xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 모델 객체\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    ## Embedding -> 카테고리컬 단어값을 고차원으로 바꾸는 것(우리는 원핫을 사용함)\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps) ## 단어 개수, 변환하고자 하는 임베딩 차원, 한 문장의 길이\n",
        "    self.dropout = Dropout(0.2) ## 과적합을 방지하기 위한 하이퍼파라미터. 임의로 20% 뉴런을 잡아서 비활성화 시킴\n",
        "    self.lstm = LSTM(units, return_state=True) ## 최종 히든 스테이트를 얻어야 벡터 콘텍스트에 넣을 수 있음.\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs) ## 임베딩 세팅\n",
        "    x = self.dropout(x) ## 과적합 방지 파라미터 세팅\n",
        "    x, hidden_state, cell_state =self.lstm(x) ## 정답, 히든 스테이트, 셀 스테이트\n",
        "\n",
        "    return [hidden_state, cell_state]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R5R2hw6y1KOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps)\n",
        "    self.dropout = Dropout(0.2)\n",
        "    self.lstm = LSTM(units,\n",
        "                     return_state=True, # 스테이트값을 알아야 다음 셀에서 진행 가능\n",
        "                     return_sequences=True ## 각 유닛의 스테이트값을 다 얻어서 결과를 얻어야 하므로\n",
        "    )\n",
        "    self.dense = Dense(vocab_size, activation='softmax') ## 결과를 얻기 위한 출력층\n",
        "\n",
        "  def call(self, inputs, initial_state): ## initial_state는 encoder의 출력값\n",
        "    x = self.embedding(inputs)\n",
        "    x = self.dropout(x)\n",
        "    x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n",
        "    x = self.dense(x) # 최종 결과값은 출력층을 거쳐 결과를 낸다\n",
        "\n",
        "    return x, hidden_state, cell_state\n",
        "\n"
      ],
      "metadata": {
        "id": "DFGcj2rI2w-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seq(tf.keras.Model):\n",
        "  def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "    super(Seq2seq, self).__init__()\n",
        "\n",
        "    self.start_token = start_token\n",
        "    self.end_token = end_token\n",
        "    self.time_steps = time_steps # 문장의 길이(30)\n",
        "\n",
        "    self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "    self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "\n",
        "  def call(self, inputs, training=True): ## training: 학습용, 예측용 구별\n",
        "    if training: ## 학습일 땐,\n",
        "      encoder_inputs, decoder_inputs = inputs ## 인코더, 디코더 모두 동일한 입력값 넣는다.\n",
        "      context_vector = self.encoder(encoder_inputs) ## 인코더에 넣어서 벡터 얻어냄\n",
        "      decoder_outputs, _, _ = self.decoder(inputs=decoder_inputs, initial_state=context_vector) ## 얻어낸 인코더의 벡터값을 디코더에 사용\n",
        "\n",
        "      return decoder_outputs\n",
        "\n",
        "    else: ## 예측일 땐,\n",
        "      context_vector = self.encoder(inputs) ##\n",
        "      target_seq = tf.constant([[self.start_token]], dtype=tf.float32) ## 첫번째는 무조건 <START>,\n",
        "      results = tf.TensorArray(tf.int32, self.time_steps) ## 결과 배열. 그래프 그리기 위해 텐서 배열로 담는다.\n",
        "\n",
        "      for i in tf.range(self.time_steps):\n",
        "        decoder_output, decoder_hidden, decoder_cell = self.decoder(target_seq, initial_state=context_vector)\n",
        "        decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32)\n",
        "        decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "        results = results.write(i, decoder_output)\n",
        "\n",
        "        if decoder_output == self.end_token:\n",
        "          break\n",
        "\n",
        "        target_seq = decoder_output\n",
        "        context_vector = [decoder_hidden, decoder_cell]\n",
        "\n",
        "      return tf.reshape(results.stack(), shape=(1, self.time_steps))"
      ],
      "metadata": {
        "id": "-HY46OQZ4CPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 16\n",
        "EMBEDDING_DIM = 100\n",
        "TIME_STEPS = MAX_LENGTH\n",
        "\n",
        "START_TOKEN = tokenizer.word_index['<START>']\n",
        "END_TOKEN = tokenizer.word_index['<END>']\n",
        "UNITS = 128\n",
        "\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "DATA_LENGTH = len(questions)\n",
        "SAMPLE_SIZE = 3\n",
        "NUM_EPOCHS = 10\n"
      ],
      "metadata": {
        "id": "Iv-9wiV96bsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "NcxlNqxp7Zyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'sample-checkpoint.h5'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                             save_best_only=True,\n",
        "                             monitor='loss',\n",
        "                             verbose=1,\n",
        "                             save_weights_only=True)"
      ],
      "metadata": {
        "id": "S9zXYwO67rv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq = Seq2seq(UNITS, VOCAB_SIZE, EMBEDDING_DIM, TIME_STEPS, START_TOKEN, END_TOKEN)\n",
        "seq2seq.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "-UVmgJdH79OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(model, question_inputs) :\n",
        "  results = model(inputs=question_inputs, training=False)\n",
        "  results = np.asarray(results).reshape(-1)\n",
        "  return results\n",
        "\n"
      ],
      "metadata": {
        "id": "TeHUDfuQdx_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"processing epoch : {epoch * 10 + 1} ...\")\n",
        "  seq2seq.fit([question_padded, answer_in_padded], answer_out_one_hot, epochs=10, batch_size=BATCH_SIZE, callbacks=[checkpoint])\n",
        "\n",
        "  samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
        "\n",
        "  for idx in samples:\n",
        "    question_inputs = question_padded[idx]\n",
        "    results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
        "\n",
        "    results = convert_index_to_text(results, END_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9o5CBfDeKP6",
        "outputId": "b4e96cfd-f62b-472e-9811-dd666e4a8ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing epoch : 1 ...\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 3.2577 - acc: 0.7767\n",
            "Epoch 1: loss improved from inf to 3.25767, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 12s 87ms/step - loss: 3.2577 - acc: 0.7767\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.3077 - acc: 0.8013\n",
            "Epoch 2: loss improved from 3.25767 to 1.30768, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 93ms/step - loss: 1.3077 - acc: 0.8013\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.2311 - acc: 0.8068\n",
            "Epoch 3: loss improved from 1.30768 to 1.23105, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 91ms/step - loss: 1.2311 - acc: 0.8068\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.1485 - acc: 0.8196\n",
            "Epoch 4: loss improved from 1.23105 to 1.14854, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 94ms/step - loss: 1.1485 - acc: 0.8196\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.1068 - acc: 0.8312\n",
            "Epoch 5: loss improved from 1.14854 to 1.10680, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 83ms/step - loss: 1.1068 - acc: 0.8312\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.0744 - acc: 0.8345\n",
            "Epoch 6: loss improved from 1.10680 to 1.07438, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 97ms/step - loss: 1.0744 - acc: 0.8345\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.0478 - acc: 0.8354\n",
            "Epoch 7: loss improved from 1.07438 to 1.04777, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 85ms/step - loss: 1.0478 - acc: 0.8354\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.0254 - acc: 0.8362\n",
            "Epoch 8: loss improved from 1.04777 to 1.02538, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 98ms/step - loss: 1.0254 - acc: 0.8362\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.0067 - acc: 0.8368\n",
            "Epoch 9: loss improved from 1.02538 to 1.00672, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 98ms/step - loss: 1.0067 - acc: 0.8368\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.9910 - acc: 0.8380\n",
            "Epoch 10: loss improved from 1.00672 to 0.99101, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 90ms/step - loss: 0.9910 - acc: 0.8380\n",
            "processing epoch : 11 ...\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.9756 - acc: 0.8392\n",
            "Epoch 1: loss improved from 0.99101 to 0.97558, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.9756 - acc: 0.8392\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.9609 - acc: 0.8409\n",
            "Epoch 2: loss improved from 0.97558 to 0.96088, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 94ms/step - loss: 0.9609 - acc: 0.8409\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.9462 - acc: 0.8414\n",
            "Epoch 3: loss improved from 0.96088 to 0.94621, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 83ms/step - loss: 0.9462 - acc: 0.8414\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.9322 - acc: 0.8419\n",
            "Epoch 4: loss improved from 0.94621 to 0.93224, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 98ms/step - loss: 0.9322 - acc: 0.8419\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.9182 - acc: 0.8435\n",
            "Epoch 5: loss improved from 0.93224 to 0.91816, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 79ms/step - loss: 0.9182 - acc: 0.8435\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.9043 - acc: 0.8445\n",
            "Epoch 6: loss improved from 0.91816 to 0.90426, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.9043 - acc: 0.8445\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8910 - acc: 0.8455\n",
            "Epoch 7: loss improved from 0.90426 to 0.89097, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 89ms/step - loss: 0.8910 - acc: 0.8455\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8787 - acc: 0.8465\n",
            "Epoch 8: loss improved from 0.89097 to 0.87868, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 85ms/step - loss: 0.8787 - acc: 0.8465\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8662 - acc: 0.8471\n",
            "Epoch 9: loss improved from 0.87868 to 0.86618, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 89ms/step - loss: 0.8662 - acc: 0.8471\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8538 - acc: 0.8480\n",
            "Epoch 10: loss improved from 0.86618 to 0.85384, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 84ms/step - loss: 0.8538 - acc: 0.8480\n",
            "processing epoch : 21 ...\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8421 - acc: 0.8489\n",
            "Epoch 1: loss improved from 0.85384 to 0.84207, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.8421 - acc: 0.8489\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8296 - acc: 0.8503\n",
            "Epoch 2: loss improved from 0.84207 to 0.82956, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 8s 121ms/step - loss: 0.8296 - acc: 0.8503\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8176 - acc: 0.8516\n",
            "Epoch 3: loss improved from 0.82956 to 0.81761, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 83ms/step - loss: 0.8176 - acc: 0.8516\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8056 - acc: 0.8527\n",
            "Epoch 4: loss improved from 0.81761 to 0.80556, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 93ms/step - loss: 0.8056 - acc: 0.8527\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7927 - acc: 0.8544\n",
            "Epoch 5: loss improved from 0.80556 to 0.79275, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 83ms/step - loss: 0.7927 - acc: 0.8544\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7806 - acc: 0.8555\n",
            "Epoch 6: loss improved from 0.79275 to 0.78057, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 88ms/step - loss: 0.7806 - acc: 0.8555\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7678 - acc: 0.8580\n",
            "Epoch 7: loss improved from 0.78057 to 0.76776, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 86ms/step - loss: 0.7678 - acc: 0.8580\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7554 - acc: 0.8589\n",
            "Epoch 8: loss improved from 0.76776 to 0.75542, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 93ms/step - loss: 0.7554 - acc: 0.8589\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7432 - acc: 0.8617\n",
            "Epoch 9: loss improved from 0.75542 to 0.74324, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 85ms/step - loss: 0.7432 - acc: 0.8617\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7301 - acc: 0.8631\n",
            "Epoch 10: loss improved from 0.74324 to 0.73007, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 95ms/step - loss: 0.7301 - acc: 0.8631\n",
            "processing epoch : 31 ...\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7202 - acc: 0.8648\n",
            "Epoch 1: loss improved from 0.73007 to 0.72016, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 94ms/step - loss: 0.7202 - acc: 0.8648\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7062 - acc: 0.8671\n",
            "Epoch 2: loss improved from 0.72016 to 0.70621, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 93ms/step - loss: 0.7062 - acc: 0.8671\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6942 - acc: 0.8695\n",
            "Epoch 3: loss improved from 0.70621 to 0.69425, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 84ms/step - loss: 0.6942 - acc: 0.8695\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6814 - acc: 0.8712\n",
            "Epoch 4: loss improved from 0.69425 to 0.68144, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 94ms/step - loss: 0.6814 - acc: 0.8712\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6688 - acc: 0.8732\n",
            "Epoch 5: loss improved from 0.68144 to 0.66884, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 79ms/step - loss: 0.6688 - acc: 0.8732\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6561 - acc: 0.8758\n",
            "Epoch 6: loss improved from 0.66884 to 0.65614, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 94ms/step - loss: 0.6561 - acc: 0.8758\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6425 - acc: 0.8781\n",
            "Epoch 7: loss improved from 0.65614 to 0.64253, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 77ms/step - loss: 0.6425 - acc: 0.8781\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6299 - acc: 0.8797\n",
            "Epoch 8: loss improved from 0.64253 to 0.62992, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 81ms/step - loss: 0.6299 - acc: 0.8797\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6178 - acc: 0.8824\n",
            "Epoch 9: loss improved from 0.62992 to 0.61776, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 88ms/step - loss: 0.6178 - acc: 0.8824\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6054 - acc: 0.8850\n",
            "Epoch 10: loss improved from 0.61776 to 0.60537, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.6054 - acc: 0.8850\n",
            "processing epoch : 41 ...\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5930 - acc: 0.8872\n",
            "Epoch 1: loss improved from 0.60537 to 0.59301, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 86ms/step - loss: 0.5930 - acc: 0.8872\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5801 - acc: 0.8902\n",
            "Epoch 2: loss improved from 0.59301 to 0.58010, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 78ms/step - loss: 0.5801 - acc: 0.8902\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5686 - acc: 0.8922\n",
            "Epoch 3: loss improved from 0.58010 to 0.56857, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 94ms/step - loss: 0.5686 - acc: 0.8922\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5563 - acc: 0.8948\n",
            "Epoch 4: loss improved from 0.56857 to 0.55633, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.5563 - acc: 0.8948\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5448 - acc: 0.8976\n",
            "Epoch 5: loss improved from 0.55633 to 0.54475, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 93ms/step - loss: 0.5448 - acc: 0.8976\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5338 - acc: 0.9001\n",
            "Epoch 6: loss improved from 0.54475 to 0.53376, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 78ms/step - loss: 0.5338 - acc: 0.9001\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5225 - acc: 0.9018\n",
            "Epoch 7: loss improved from 0.53376 to 0.52255, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 85ms/step - loss: 0.5225 - acc: 0.9018\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5108 - acc: 0.9058\n",
            "Epoch 8: loss improved from 0.52255 to 0.51079, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.5108 - acc: 0.9058\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5012 - acc: 0.9081\n",
            "Epoch 9: loss improved from 0.51079 to 0.50120, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.5012 - acc: 0.9081\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4900 - acc: 0.9099\n",
            "Epoch 10: loss improved from 0.50120 to 0.48998, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 93ms/step - loss: 0.4900 - acc: 0.9099\n",
            "processing epoch : 51 ...\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4801 - acc: 0.9120\n",
            "Epoch 1: loss improved from 0.48998 to 0.48005, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 96ms/step - loss: 0.4801 - acc: 0.9120\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4685 - acc: 0.9149\n",
            "Epoch 2: loss improved from 0.48005 to 0.46846, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 88ms/step - loss: 0.4685 - acc: 0.9149\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4595 - acc: 0.9161\n",
            "Epoch 3: loss improved from 0.46846 to 0.45954, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 95ms/step - loss: 0.4595 - acc: 0.9161\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4498 - acc: 0.9184\n",
            "Epoch 4: loss improved from 0.45954 to 0.44976, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 85ms/step - loss: 0.4498 - acc: 0.9184\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4407 - acc: 0.9211\n",
            "Epoch 5: loss improved from 0.44976 to 0.44070, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 92ms/step - loss: 0.4407 - acc: 0.9211\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4318 - acc: 0.9223\n",
            "Epoch 6: loss improved from 0.44070 to 0.43175, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 83ms/step - loss: 0.4318 - acc: 0.9223\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4230 - acc: 0.9230\n",
            "Epoch 7: loss improved from 0.43175 to 0.42299, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 97ms/step - loss: 0.4230 - acc: 0.9230\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4144 - acc: 0.9250\n",
            "Epoch 8: loss improved from 0.42299 to 0.41441, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 89ms/step - loss: 0.4144 - acc: 0.9250\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4060 - acc: 0.9272\n",
            "Epoch 9: loss improved from 0.41441 to 0.40598, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 94ms/step - loss: 0.4060 - acc: 0.9272\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3992 - acc: 0.9282\n",
            "Epoch 10: loss improved from 0.40598 to 0.39924, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 80ms/step - loss: 0.3992 - acc: 0.9282\n",
            "processing epoch : 61 ...\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3917 - acc: 0.9307\n",
            "Epoch 1: loss improved from 0.39924 to 0.39171, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 100ms/step - loss: 0.3917 - acc: 0.9307\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3843 - acc: 0.9314\n",
            "Epoch 2: loss improved from 0.39171 to 0.38431, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 81ms/step - loss: 0.3843 - acc: 0.9314\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3766 - acc: 0.9326\n",
            "Epoch 3: loss improved from 0.38431 to 0.37656, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 101ms/step - loss: 0.3766 - acc: 0.9326\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3698 - acc: 0.9346\n",
            "Epoch 4: loss improved from 0.37656 to 0.36980, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 81ms/step - loss: 0.3698 - acc: 0.9346\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3637 - acc: 0.9357\n",
            "Epoch 5: loss improved from 0.36980 to 0.36374, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 87ms/step - loss: 0.3637 - acc: 0.9357\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3573 - acc: 0.9367\n",
            "Epoch 6: loss improved from 0.36374 to 0.35731, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 89ms/step - loss: 0.3573 - acc: 0.9367\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3514 - acc: 0.9379\n",
            "Epoch 7: loss improved from 0.35731 to 0.35136, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 81ms/step - loss: 0.3514 - acc: 0.9379\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3452 - acc: 0.9390\n",
            "Epoch 8: loss improved from 0.35136 to 0.34524, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 87ms/step - loss: 0.3452 - acc: 0.9390\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3407 - acc: 0.9385\n",
            "Epoch 9: loss improved from 0.34524 to 0.34072, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 86ms/step - loss: 0.3407 - acc: 0.9385\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3363 - acc: 0.9401\n",
            "Epoch 10: loss improved from 0.34072 to 0.33632, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 99ms/step - loss: 0.3363 - acc: 0.9401\n",
            "processing epoch : 71 ...\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3299 - acc: 0.9413\n",
            "Epoch 1: loss improved from 0.33632 to 0.32994, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 87ms/step - loss: 0.3299 - acc: 0.9413\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3243 - acc: 0.9416\n",
            "Epoch 2: loss improved from 0.32994 to 0.32434, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 94ms/step - loss: 0.3243 - acc: 0.9416\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3195 - acc: 0.9435\n",
            "Epoch 3: loss improved from 0.32434 to 0.31947, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.3195 - acc: 0.9435\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3152 - acc: 0.9433\n",
            "Epoch 4: loss improved from 0.31947 to 0.31521, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 96ms/step - loss: 0.3152 - acc: 0.9433\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3114 - acc: 0.9441\n",
            "Epoch 5: loss improved from 0.31521 to 0.31139, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 85ms/step - loss: 0.3114 - acc: 0.9441\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3060 - acc: 0.9450\n",
            "Epoch 6: loss improved from 0.31139 to 0.30599, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 91ms/step - loss: 0.3060 - acc: 0.9450\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3024 - acc: 0.9461\n",
            "Epoch 7: loss improved from 0.30599 to 0.30240, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 85ms/step - loss: 0.3024 - acc: 0.9461\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3000 - acc: 0.9468\n",
            "Epoch 8: loss improved from 0.30240 to 0.29999, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 95ms/step - loss: 0.3000 - acc: 0.9468\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2951 - acc: 0.9473\n",
            "Epoch 9: loss improved from 0.29999 to 0.29509, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 83ms/step - loss: 0.2951 - acc: 0.9473\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2918 - acc: 0.9475\n",
            "Epoch 10: loss improved from 0.29509 to 0.29175, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 97ms/step - loss: 0.2918 - acc: 0.9475\n",
            "processing epoch : 81 ...\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2878 - acc: 0.9484\n",
            "Epoch 1: loss improved from 0.29175 to 0.28784, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 90ms/step - loss: 0.2878 - acc: 0.9484\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2847 - acc: 0.9486\n",
            "Epoch 2: loss improved from 0.28784 to 0.28472, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 98ms/step - loss: 0.2847 - acc: 0.9486\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2807 - acc: 0.9500\n",
            "Epoch 3: loss improved from 0.28472 to 0.28068, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 86ms/step - loss: 0.2807 - acc: 0.9500\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2783 - acc: 0.9501\n",
            "Epoch 4: loss improved from 0.28068 to 0.27828, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 91ms/step - loss: 0.2783 - acc: 0.9501\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2744 - acc: 0.9507\n",
            "Epoch 5: loss improved from 0.27828 to 0.27443, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.2744 - acc: 0.9507\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2709 - acc: 0.9512\n",
            "Epoch 6: loss improved from 0.27443 to 0.27090, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 100ms/step - loss: 0.2709 - acc: 0.9512\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2688 - acc: 0.9516\n",
            "Epoch 7: loss improved from 0.27090 to 0.26877, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 90ms/step - loss: 0.2688 - acc: 0.9516\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2657 - acc: 0.9516\n",
            "Epoch 8: loss improved from 0.26877 to 0.26575, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 99ms/step - loss: 0.2657 - acc: 0.9516\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2638 - acc: 0.9520\n",
            "Epoch 9: loss improved from 0.26575 to 0.26376, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 87ms/step - loss: 0.2638 - acc: 0.9520\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2608 - acc: 0.9522\n",
            "Epoch 10: loss improved from 0.26376 to 0.26080, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 95ms/step - loss: 0.2608 - acc: 0.9522\n",
            "processing epoch : 91 ...\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2581 - acc: 0.9527\n",
            "Epoch 1: loss improved from 0.26080 to 0.25812, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 84ms/step - loss: 0.2581 - acc: 0.9527\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2571 - acc: 0.9525\n",
            "Epoch 2: loss improved from 0.25812 to 0.25705, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 97ms/step - loss: 0.2571 - acc: 0.9525\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2534 - acc: 0.9536\n",
            "Epoch 3: loss improved from 0.25705 to 0.25343, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 86ms/step - loss: 0.2534 - acc: 0.9536\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2516 - acc: 0.9538\n",
            "Epoch 4: loss improved from 0.25343 to 0.25163, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 94ms/step - loss: 0.2516 - acc: 0.9538\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2492 - acc: 0.9545\n",
            "Epoch 5: loss improved from 0.25163 to 0.24918, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 5s 85ms/step - loss: 0.2492 - acc: 0.9545\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2472 - acc: 0.9545\n",
            "Epoch 6: loss improved from 0.24918 to 0.24720, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 97ms/step - loss: 0.2472 - acc: 0.9545\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2481 - acc: 0.9546\n",
            "Epoch 7: loss did not improve from 0.24720\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.2481 - acc: 0.9546\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2444 - acc: 0.9548\n",
            "Epoch 8: loss improved from 0.24720 to 0.24444, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 97ms/step - loss: 0.2444 - acc: 0.9548\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2424 - acc: 0.9548\n",
            "Epoch 9: loss improved from 0.24444 to 0.24236, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 98ms/step - loss: 0.2424 - acc: 0.9548\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2394 - acc: 0.9554\n",
            "Epoch 10: loss improved from 0.24236 to 0.23937, saving model to sample-checkpoint.h5\n",
            "63/63 [==============================] - 6s 97ms/step - loss: 0.2394 - acc: 0.9554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq"
      ],
      "metadata": {
        "id": "IEdBJTVseKWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b9434b-ca5f-4342-f08f-9804444258ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Seq2seq at 0x7f97c21545e0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq.save_weights('test')"
      ],
      "metadata": {
        "id": "97zvTHK05q9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_question(sentence):\n",
        "  sentence = clean_and_morph(sentence)\n",
        "  question_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "  question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "\n",
        "  return question_padded"
      ],
      "metadata": {
        "id": "IXSxEb7t6gII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chatbot(question):\n",
        "  question_inputs = make_question(question)\n",
        "  results = make_prediction(seq2seq, question_inputs)\n",
        "  results = convert_index_to_text(results, '<END>')\n",
        "  return results"
      ],
      "metadata": {
        "id": "IoS1WdFc72rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_input = input('\\n말을 걸어 주세요.')\n",
        "  if user_input == 'q':\n",
        "    break\n",
        "  answer = run_chatbot(user_input)\n",
        "  print(f'챗봇 응답:{answer}\\n')"
      ],
      "metadata": {
        "id": "3g3yDE8V8Te9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq3 = Seq2seq(UNITS, VOCAB_SIZE, EMBEDDING_DIM, TIME_STEPS, START_TOKEN, END_TOKEN)"
      ],
      "metadata": {
        "id": "bm31ni689J6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seq2seq.save_weights('aaa')\n",
        "seq2seq3.load_weights('aaa')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUea34Y7MMRL",
        "outputId": "1e78b9bf-20db-46ff-8613-53bfdac5f73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f97c10bec20>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chatbot(question):\n",
        "  question_inputs = make_question(question)\n",
        "  results = make_prediction(seq2seq3, question_inputs)\n",
        "  results = convert_index_to_text(results, '<END>')\n",
        "  return results"
      ],
      "metadata": {
        "id": "Q_3YzplEMepL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_question(sentence):\n",
        "  sentence = clean_and_morph(sentence)\n",
        "  question_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "  question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "\n",
        "  return question_padded"
      ],
      "metadata": {
        "id": "fZ-jFznBMl_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_chatbot(\"아오 치치!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lVua58PMMoYZ",
        "outputId": "14917eab-bc68-46b8-b128-b8748bcfbc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'저 도 궁금하네요 <END>                           '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(tokenizer, 'tokenizer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5asvqMUUNzq",
        "outputId": "a68a0b01-c1ae-4faf-a940-b23124badb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tokenizer']"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    }
  ]
}